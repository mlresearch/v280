---
title: 'Taming Sensitive Weights : Noise Perturbation Fine-tuning for Robust LLM Quantization'
openreview: VehapTAftQ
abstract: Quantization is a critical step to enable efficient LLM serving under limited
  resource. However, previous research observes that certain weights in the LLM, known
  as outliers, are significantly sensitive to quantization noises. Existing quantization
  methods leave these outliers as floating points or higher precisions to retain performance,
  posting challenges on the efficient hardware deployment of the mixed-precision model.
  This work investigates an alternative way to tame the sensitive weightsâ€™ impact
  on the quantization error, by reducing the loss Hessian trace with respect to outliers
  through an efficient fine-tuning process. We propose Noise Perturbation Fine-tuning
  (NPFT), which identifies outlier weights and add random weight perturbations on
  the outliers as the model going through a PEFT optimization. NPFT tames the sensitivity
  of outlier weights so that the quantized model performance can be improved without
  special treatment to the outliers. When applied to OPT and LLaMA models, our NPFT
  method achieves stable performance improvements for both uniform and non-uniform
  quantizers, while also offering better inference efficiency. Notably, the simplest
  RTN can achieve performance on par with GPTQ using our NPFT on LLaMA2-7B-4bits benchmark.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25d
month: 0
tex_title: 'Taming Sensitive Weights : Noise Perturbation Fine-tuning for Robust LLM
  Quantization'
firstpage: 884
lastpage: 896
page: 884-896
order: 884
cycles: false
bibtex_author: WANG, DONGWEI and Yang, Huanrui
author:
- given: DONGWEI
  family: WANG
- given: Huanrui
  family: Yang
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/wang25d/wang25d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
