---
title: 'Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank
  Gradients'
openreview: KTAPk6c2hl
abstract: 'Training Large Language Models (LLMs) is memory-intensive due to the large
  number of parameters and associated optimization states. GaLore, a recent method,
  reduces memory usage by projecting weight gradients into a low-rank subspace without
  compromising performance. However, GaLore relies on time-consuming Singular Value
  Decomposition (SVD) operations to identify the subspace, and the frequent subspace
  updates lead to significant training time overhead. Moreover, GaLore offers minimal
  improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning
  scenarios. To address these limitations, we introduce Q-GaLore, a novel approach
  that substantially reduces memory usage by combining quantization and low-rank projection,
  surpassing the benefits of GaLore. Our method is based on two key observations:
  (i) the gradient subspace exhibits diverse properties, with some layers converging
  early in training while others are subject to frequent changes; (ii) the projection
  matrices are highly resilient to low-bit quantization. Leveraging these insights,
  Q-GaLore adaptively updates the gradient subspace based on its convergence statistics,
  achieving comparable performance while significantly reducing the number of SVD
  operations. We maintain the projection matrices in INT4 format for aggressive memory
  conservation and preserve weights in INT8 format, incorporating stochastic rounding
  to capture accumulated gradient information. This approach enables a high-precision
  training trajectory using only low-precision weights. We demonstrate that Q-GaLore
  achieves highly competitive pre-training and fine-tuning performance with exceptional
  memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model
  from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory, showcasing its
  exceptional memory efficiency and practicality. At fine-tuning, it reduces memory
  consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming
  QLoRA (by up to 5.19 on MMLU) at the same memory cost.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang25a
month: 0
tex_title: 'Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank
  Gradients'
firstpage: 1035
lastpage: 1050
page: 1035-1050
order: 1035
cycles: false
bibtex_author: Zhang, Zhenyu and JAISWAL, AJAY KUMAR and Yin, Lu and Liu, Shiwei and
  Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang
author:
- given: Zhenyu
  family: Zhang
- given: AJAY KUMAR
  family: JAISWAL
- given: Lu
  family: Yin
- given: Shiwei
  family: Liu
- given: Jiawei
  family: Zhao
- given: Yuandong
  family: Tian
- given: Zhangyang
  family: Wang
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/zhang25a/zhang25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
