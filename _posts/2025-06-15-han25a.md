---
title: 'You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at Inference
  Time'
openreview: XYMWd1wNlf
abstract: 'Deep neural networks are prone to various bias issues, jeopardizing their
  applications for high-stake decision-making. Existing fairness methods typically
  offer a fixed accuracy-fairness trade-off, since the weight of the well-trained
  model is a fixed point (fairness-optimum) in the weight space. Nevertheless, more
  flexible accuracy-fairness trade-offs at inference time are practically desired
  since: 1) stakes of the same downstream task can vary for different individuals,
  and 2) different regions have diverse laws or regularization for fairness. If using
  the previous fairness methods, we have to train multiple models, each offering a
  specific level of accuracy-fairness trade-off. This is often computationally expensive,
  time-consuming, and difficult to deploy, making it less practical for real-world
  applications. To address this problem, we propose You Only Debias Once (YODO) to
  achieve in-situ flexible accuracy-fairness trade-offs at inference time, using a
  single model that trained only once. Instead of pursuing one individual fixed point
  (fairness-optimum) in the weight space, we aim to find a ”line” in the weight space
  that connects the accuracy-optimum and fairness-optimum points using a single model.
  Points (models) on this line implement varying levels of accuracy-fairness trade-offs.
  At inference time, by manually selecting the specific position of the learned “line”,
  our proposed method can achieve arbitrary accuracy-fairness trade-offs for different
  end-users and scenarios. Experimental results on tabular and image datasets show
  that YODO achieves flexible trade-offs between model accuracy and fairness, at ultra-low
  overheads. For example, if we need $100$ levels of trade-off on the \acse dataset,
  YODO takes $3.53$ seconds while training $100$ fixed models consumes $425$ seconds.
  The code is available at https://github.com/ahxt/yodo.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han25a
month: 0
tex_title: 'You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at
  Inference Time'
firstpage: 780
lastpage: 809
page: 780-809
order: 780
cycles: false
bibtex_author: Han, Xiaotian and Chen, Tianlong and Zhou, Kaixiong and Jiang, Zhimeng
  and Wang, Zhangyang and Hu, Xia
author:
- given: Xiaotian
  family: Han
- given: Tianlong
  family: Chen
- given: Kaixiong
  family: Zhou
- given: Zhimeng
  family: Jiang
- given: Zhangyang
  family: Wang
- given: Xia
  family: Hu
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/han25a/han25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
