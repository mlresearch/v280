---
title: Approximate Nullspace Augmented Finetuning for Robust Vision Transformers
openreview: zH3Zwx3dLQ
abstract: Enhancing the robustness of deep learning models, particularly in the realm
  of vision transformers (ViTs), is crucial for their real-world deployment. In this
  work, we provide a finetuning approach to enhance the robustness of vision transformers
  inspired by the concept of nullspace from linear algebra. Our investigation centers
  on whether a vision transformer can exhibit resilience to input variations akin
  to the nullspace property in linear mappings, implying that perturbations sampled
  from this nullspace do not influence the modelâ€™s output when added to the input.
  We start from the observation that many existing ViTs satisfy this property because
  a non-trivial nullspace exists in their patch embedding layers. Secondly, as nullspace
  is a concept associated with linear algebra, we demonstrate that it is possible
  to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing
  an optimisation strategy. Finally, we propose a fine-tuning strategy for ViTs wherein
  we augment the training data with synthesized approximate nullspace noise. After
  finetuning, we find that the model demonstrates robustness to adversarial and natural
  image perbutations alike.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu25a
month: 0
tex_title: Approximate Nullspace Augmented Finetuning for Robust Vision Transformers
firstpage: 1
lastpage: 23
page: 1-23
order: 1
cycles: false
bibtex_author: Liu, Haoyang and Singh, Aditya and Li, Yijiang and Wang, Haohan
author:
- given: Haoyang
  family: Liu
- given: Aditya
  family: Singh
- given: Yijiang
  family: Li
- given: Haohan
  family: Wang
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/liu25a/liu25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
