---
title: A unified framework for Sparse plus Low-Rank Matrix Decomposition for LLMs
openreview: hyN75SAJTI
abstract: The impressive capabilities of large foundation models come at a cost of
  substantial computing resources to serve them. Compressing these pre-trained models
  is of practical interest as it can democratize deploying them to the machine learning
  community at large by lowering the costs associated with inference. A promising
  compression scheme is to decompose foundation modelsâ€™ dense weights into a sum of
  sparse plus low-rank matrices. In this paper, we design a unified framework coined
  $\texttt{HASSLE-free}$ for (semi-structured) sparse plus low-rank matrix decomposition
  of foundation models. Our framework introduces the local layer-wise reconstruction
  error objective for this decomposition, we demonstrate that prior work solves a
  relaxation of this optimization problem; and we provide efficient and scalable methods
  to minimize the $\textit{exact}$ introduced optimization problem.  $\texttt{HASSLE-free}$
  substantially outperforms state-of-the-art methods in terms of the introduced objective
  and a wide range of LLM evaluation benchmarks. For the Llama3-8B model with a 2:4
  sparsity component plus a 64-rank component decomposition, a compression scheme
  for which recent work shows important inference acceleration on GPUs, $\texttt{HASSLE-free}$
  reduces the test perplexity by $18$% for the WikiText-2 dataset and reduces the
  gap (compared to the dense model) of the average of eight popular zero-shot tasks
  by $28$% compared to existing methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: makni25a
month: 0
tex_title: A unified framework for Sparse plus Low-Rank Matrix Decomposition for LLMs
firstpage: 484
lastpage: 499
page: 484-499
order: 484
cycles: false
bibtex_author: Makni, Mehdi and Behdin, Kayhan and Xu, Zheng and Ponomareva, Natalia
  and Mazumder, Rahul
author:
- given: Mehdi
  family: Makni
- given: Kayhan
  family: Behdin
- given: Zheng
  family: Xu
- given: Natalia
  family: Ponomareva
- given: Rahul
  family: Mazumder
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/makni25a/makni25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
