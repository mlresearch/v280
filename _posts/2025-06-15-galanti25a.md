---
title: SGD with Weight Decay Secretly Minimizes the Ranks of Your Neural Networks
openreview: 0LzE9AROwD
abstract: We explore the implicit bias of Stochastic Gradient Descent (SGD) toward
  learning low-rank weight matrices during the training of deep neural networks. Through
  theoretical analysis and empirical validation, we demonstrate that this rank-minimizing
  bias becomes more pronounced with smaller batch sizes, higher learning rates, or
  stronger weight decay. Unlike previous studies, our analysis does not rely on restrictive
  assumptions about data, convergence, optimality of the learned weight matrices,
  network architecture, making it applicable to a wide range of neural network architectures
  of any width or depth. We further show that weight decay is essential for inducing
  this low-rank bias. Finally, we empirically explore the connection between this
  bias and generalization, finding that it has a noticeable, yet marginal, effect
  on the test performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: galanti25a
month: 0
tex_title: SGD with Weight Decay Secretly Minimizes the Ranks of Your Neural Networks
firstpage: 1388
lastpage: 1412
page: 1388-1412
order: 1388
cycles: false
bibtex_author: Galanti, Tomer and Siegel, Zachary S and Gupte, Aparna and Poggio,
  Tomaso A
author:
- given: Tomer
  family: Galanti
- given: Zachary S
  family: Siegel
- given: Aparna
  family: Gupte
- given: Tomaso A
  family: Poggio
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/galanti25a/galanti25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
