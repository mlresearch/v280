---
title: How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully Connected
  Neural Networks
openreview: B936pXBrz5
abstract: Since its use in the Lottery Ticket Hypothesis, iterative magnitude pruning
  (IMP) has become a popular method for extracting sparse subnetworks that can be
  trained to high performance. Despite its success, the mechanism that drives the
  success of IMP remains unclear. One possibility is that IMP is capable of extracting
  subnetworks with good inductive biases that facilitate performance. Supporting this
  idea, recent work showed that applying IMP to fully connected neural networks (FCNs)
  leads to the emergence of local receptive fields (RFs), a feature of mammalian visual
  cortex and convolutional neural networks that facilitates image processing. However,
  it remains unclear why IMP would uncover localised features in the first place.
  Inspired by results showing that training on synthetic images with highly non-Gaussian
  statistics (e.g., sharp edges) is sufficient to drive the emergence of local RFs
  in FCNs, we hypothesize that IMP iteratively increases the non-Gaussian statistics
  of FCN representations, creating a feedback loop that enhances localization. Here,
  we demonstrate first that non-Gaussian input statistics are indeed necessary for
  IMP to discover localized RFs. We then develop a new method for measuring the effect
  of individual weights on the statistics of the FCN representations ("cavity method"),
  which allows us to show that IMP systematically increases the non-Gaussianity of
  pre-activations, leading to the formation of localised RFs. Our work, which is the
  first to study the effect of IMP on the statistics of the representations of neural
  networks, sheds parsimonious light on one way in which IMP can drive the formation
  of strong inductive biases.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: redman25a
month: 0
tex_title: How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully
  Connected Neural Networks
firstpage: 1274
lastpage: 1291
page: 1274-1291
order: 1274
cycles: false
bibtex_author: Redman, William T and Wang, Zhangyang and Ingrosso, Alessandro and
  Goldt, Sebastian
author:
- given: William T
  family: Redman
- given: Zhangyang
  family: Wang
- given: Alessandro
  family: Ingrosso
- given: Sebastian
  family: Goldt
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/redman25a/redman25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
