---
title: 'Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning Issues
  in Multi-Modal Multi-Task Learning'
openreview: IFfyezixou
abstract: 'Sparse Mixture-of-Experts (SMoE) is a promising paradigm that can be easily
  tailored for multi-task learning. Its conditional computing nature allows us to
  organically allocate relevant parts of a model for performant and efficient predictions.
  However, several under-explored pain points persist, especially when considering
  scenarios with both multiple modalities and tasks: 1. $\textit{{Modality Forgetting
  Issue.}}$ Diverse modalities may prefer conflicting optimization directions, resulting
  in ineffective learning or knowledge forgetting; 2. $\textit{{Modality Fitting Issue.}}$
  Current SMoE pipelines select a fixed number of experts for all modalities, which
  can end up over-fitting to simpler modalities or under-fitting complex modalities;
  3. $\textit{{Heterogeneous Learning Pace.}}$ The varied modality attributes, task
  resources, and objectives usually lead to distinct optimization difficulties and
  convergence. Given these issues, there is a clear need for a systematic approach
  to harmonizing multi-modal and multi-task objectives when using SMoE. We aim to
  address these pain points and propose a new $\underline{S}$parse $\underline{M}$oE
  for $\underline{M}$ulti-$\underline{M}$odal $\underline{M}$ulti-task learning, $\textit{a.k.a.}$,
  $\texttt{SM$^4$}$, which ($1$) disentangles model spaces for different modalities
  to mitigate their optimization conflicts;  ($2$) automatically determines the modality-specific
  model size to improve fitting; and ($3$) synchronizes the learning paces of disparate
  modalities and tasks based on training dynamics in SMoE like the entropy of routing
  decisions. Comprehensive experiments validate the effectiveness of $\texttt{SM$^4$}$,
  which outperforms previous state-of-the-art across $3$ task groups and $11$ different
  modalities with a clear performance margin ($\textit{e.g.}$, $\ge 1.37%$) and a
  substantial computation reduction ($46.49% \sim 98.62%$). Codes are in supplement.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peng25a
month: 0
tex_title: 'Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning
  Issues in Multi-Modal Multi-Task Learning'
firstpage: 1112
lastpage: 1145
page: 1112-1145
order: 1112
cycles: false
bibtex_author: Peng, Jie and Yun, Sukwon and Zhou, Kaixiong and Zhou, Ruida and Hartvigsen,
  Thomas and Zhang, Yanyong and Wang, Zhangyang and Chen, Tianlong
author:
- given: Jie
  family: Peng
- given: Sukwon
  family: Yun
- given: Kaixiong
  family: Zhou
- given: Ruida
  family: Zhou
- given: Thomas
  family: Hartvigsen
- given: Yanyong
  family: Zhang
- given: Zhangyang
  family: Wang
- given: Tianlong
  family: Chen
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/peng25a/peng25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
