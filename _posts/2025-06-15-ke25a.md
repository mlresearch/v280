---
title: 'Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to
  Generalize on Time Series Forecasting and Beyond'
openreview: bdOmItHgU5
abstract: The application of transformer-based models on time series forecasting (TSF)
  tasks has long been popular to study. However, many of these works fail to beat
  the simple linear residual model, and the theoretical understanding of this issue
  is still limited. In this work, we propose the first theoretical explanation of
  the inefficiency of transformers on TSF tasks. We attribute the mechanism behind
  it to {\bf Asymmetric Learning} in training attention networks. When the sign of
  the previous step is inconsistent with the sign of the current step in the next-step-prediction
  time series, attention fails to learn the residual features. This makes it difficult
  to generalize on out-of-distribution (OOD) data, especially on the sign-inconsistent
  next-step-prediction data, with the same representation pattern, whereas a linear
  residual network could easily accomplish it. We hope our theoretical insights provide
  important necessary conditions for designing the expressive and efficient transformer-based
  architecture for practitioners.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ke25a
month: 0
tex_title: 'Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail
  to Generalize on Time Series Forecasting and Beyond'
firstpage: 675
lastpage: 738
page: 675-738
order: 675
cycles: false
bibtex_author: Ke, Yekun and Liang, Yingyu and Shi, Zhenmei and Song, Zhao and Yang,
  Chiwun
author:
- given: Yekun
  family: Ke
- given: Yingyu
  family: Liang
- given: Zhenmei
  family: Shi
- given: Zhao
  family: Song
- given: Chiwun
  family: Yang
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/ke25a/ke25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
