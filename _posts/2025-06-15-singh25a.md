---
title: 'MoXCo: How I learned to stop exploring and love my local minima?'
openreview: ferFzBa9bM
abstract: Deep neural networks are well-known for their generalization capabilities,
  largely attributed to optimizersâ€™ ability to find "good" solutions in high-dimensional
  loss landscapes. This work aims to deepen the understanding of optimization specifically
  through the lens of loss landscapes. We propose a generalized framework for adaptive
  optimization that favors convergence to these "good" solutions. Our approach shifts
  the optimization paradigm from merely finding solutions quickly to discovering solutions
  that generalize well, establishing a careful balance between optimization efficiency
  and model generalization. We empirically validate our claims using two-layer, fully
  connected neural network with ReLU activation and demonstrate practical applicability
  through binary quantization of ResNets. Our numerical results demonstrate that these
  adaptive optimizers facilitate exploration leading to faster convergence speeds
  and narrow the generalization gap between stochastic gradient descent and other
  adaptive methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: singh25a
month: 0
tex_title: 'MoXCo: How I learned to stop exploring and love my local minima?'
firstpage: 521
lastpage: 544
page: 521-544
order: 521
cycles: false
bibtex_author: Singh, Esha and Sabach, Shoham and Wang, Yu-Xiang
author:
- given: Esha
  family: Singh
- given: Shoham
  family: Sabach
- given: Yu-Xiang
  family: Wang
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/singh25a/singh25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
