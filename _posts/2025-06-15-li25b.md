---
title: 'Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs
  Without Retraining'
openreview: hp7txxx8hv
abstract: To remove redundant components of large language models (LLMs) without incurring
  significant pruning costs, this work focuses on single-shot structured pruning without
  a retraining phase. We simplify the pruning process for Transformer-based LLMs by
  identifying a depth-2 pruning structure that functions independently. Additionally,
  we propose two inference-aware pruning criteria derived from the optimization perspective
  of output approximation, which outperforms traditional training-aware metrics such
  as gradient and Hessian. We also introduce a two-step reconstruction technique to
  mitigate pruning errors without model retraining. Experimental results demonstrate
  that our strategy significantly reduces pruning costs and hardware requirements
  while maintaining superior performance across various datasets and models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li25b
month: 0
tex_title: 'Greedy Output Approximation: Towards Efficient Structured Pruning for
  LLMs Without Retraining'
firstpage: 500
lastpage: 520
page: 500-520
order: 500
cycles: false
bibtex_author: Li, Jianwei and Dong, Yijun and Lei, Qi
author:
- given: Jianwei
  family: Li
- given: Yijun
  family: Dong
- given: Qi
  family: Lei
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/li25b/li25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
