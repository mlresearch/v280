---
title: 'Are all layers created equal: A neural collapse perspective'
openreview: 5eHpefiK8W
abstract: 'Understanding how features evolve layer by layer is crucial for uncovering
  the inner workings of deep neural networks. \textit{Progressive neural collapse},
  where successive layers increasingly compress within-class features and enhance
  class separation, has been primarily studied empirically in small architectures
  on simple tasks or theoretically within linear network contexts. However, its behavior
  in larger architectures and complex datasets remains underexplored. In this work,
  we extend the study of progressive neural collapse to larger models and more complex
  datasets, including clean and noisy data settings, offering a comprehensive understanding
  of its role in generalization and robustness. Our findings reveal three key insights:1.
  Layer inequality: Deeper layers significantly enhance neural collapse and play a
  vital role in generalization but are also more susceptible to memorization. 2. Depth-dependent
  behavior: In deeper models, middle layers contribute minimally due to a diminished
  neural collapse enhancement leading to redundancy and limited generalization improvements,
  which validates the effectiveness of layer pruning. 3. Architectural differences:
  Transformer models outperform convolutional models in enhancing neural collapse
  on larger datasets and exhibit greater robustness to memorization, with deeper Transformers
  reducing memorization while deeper convolutional models show the opposite trend.
  These findings provide new insights into the hierarchical roles of layers and their
  interplay with architectural design, shedding light on how deep neural networks
  process data and generalize across challenging conditions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou25a
month: 0
tex_title: 'Are all layers created equal: A neural collapse perspective'
firstpage: 1307
lastpage: 1327
page: 1307-1327
order: 1307
cycles: false
bibtex_author: Zhou, Jinxin and Jiang, Jiachen and Zhu, Zhihui
author:
- given: Jinxin
  family: Zhou
- given: Jiachen
  family: Jiang
- given: Zhihui
  family: Zhu
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/zhou25a/zhou25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
