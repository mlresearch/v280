---
title: Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning
openreview: 2sThreW73a
abstract: Multimodal learning has recently gained significant popularity, demonstrating
  impressive performance across various zero-shot classification tasks and a range
  of perceptive and generative applications. Models such as Contrastive Languageâ€“Image
  Pretraining (CLIP) are designed to bridge different modalities, such as images and
  text, by learning a shared representation space through contrastive learning. Despite
  their success, the working mechanisms of multimodal learning remain poorly understood.
  Notably, these models often exhibit a \emph{modality gap}, where different modalities
  occupy distinct regions within the shared representation space. In this work, we
  conduct an in-depth analysis of the emergence of modality gap by characterizing
  the gradient flow learning dynamics. Specifically, we identify the critical roles
  of mismatched data pairs and a learnable temperature parameter in causing and perpetuating
  the modality gap during training. Furthermore, our theoretical insights are validated
  through experiments on practical CLIP models. These findings provide principled
  guidance for mitigating the modality gap, including strategies such as appropriate
  temperature scheduling and modality swapping. Additionally, we demonstrate that
  closing the modality gap leads to improved performance on tasks such as image-text
  retrieval.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yaras25a
month: 0
tex_title: Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning
firstpage: 1365
lastpage: 1387
page: 1365-1387
order: 1365
cycles: false
bibtex_author: Yaras, Can and Chen, Siyi and Wang, Peng and Qu, Qing
author:
- given: Can
  family: Yaras
- given: Siyi
  family: Chen
- given: Peng
  family: Wang
- given: Qing
  family: Qu
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/yaras25a/yaras25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
