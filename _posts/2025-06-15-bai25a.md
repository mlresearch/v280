---
title: Improving Neuron-level Interpretability with White-box Language Models
openreview: XPpbo0zC4Y
abstract: Neurons in auto-regressive language models like GPT-2 can be interpreted
  by analyzing their activation patterns. Recent studies have shown that techniques
  such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level
  interpretability. In our research, we are driven by the goal to fundamentally improve
  neural network interpretability by embedding sparse coding directly within the model
  architecture, rather than applying it as an afterthought. In our study, we introduce
  a white-box transformer-like architecture named Coding RAte TransformEr (CRATE),
  explicitly engineered to capture sparse, low-dimensional structures within data
  distributions. Our comprehensive experiments showcase significant improvements (up
  to 103% relative improvement) in neuron-level interpretability across a variety
  of evaluation metrics. Detailed investigations confirm that this enhanced interpretability
  is steady across different layers irrespective of the model size, underlining CRATE’s
  robust performance in enhancing neural network interpretability. Further analysis
  shows that CRATE’s increased interpretability comes from its enhanced ability to
  consistently and distinctively activate on relevant tokens. These findings point
  towards a promising direction for creating white-box foundation models that excel
  in neuron-level interpretation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bai25a
month: 0
tex_title: Improving Neuron-level Interpretability with White-box Language Models
firstpage: 810
lastpage: 836
page: 810-836
order: 810
cycles: false
bibtex_author: Bai, Hao and Ma, Yi
author:
- given: Hao
  family: Bai
- given: Yi
  family: Ma
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/bai25a/bai25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
