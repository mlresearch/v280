---
title: HSR-Enhanced Sparse Attention Acceleration
openreview: wso1gABiPZ
abstract: 'Large Language Models (LLMs) have demonstrated remarkable capabilities
  across various applications, but their performance on long-context tasks is often
  limited by the computational complexity of attention mechanisms. We introduce a
  novel approach to accelerate attention computation in LLMs, particularly for long-context
  scenarios. We leverage the inherent sparsity within attention mechanisms, both in
  conventional Softmax attention and ReLU attention (with $\mathsf{ReLU}^\alpha$ activation,
  $\alpha \in \mathbb{N}_+$), to significantly reduce the running time complexity.
  Our method employs a Half-Space Reporting (HSR) data structure to identify non-zero
  or “massively activated” entries in the attention matrix. We present theoretical
  analyses for two key scenarios: generation decoding and prompt prefilling. Our approach
  achieves a running time of $O(mn^{4/5})$ significantly faster than the naive approach
  $O(mn)$ for generation decoding, where $n$ is the context length, $m$ is the query
  length, and $d$ is the hidden dimension. We can also reduce the running time for
  prompt prefilling from $O(mn)$ to $O(mn^{1 - 1 / \lfloor d/2\rfloor} + mn^{4/5})$.
  Our method introduces only provably negligible error for Softmax attention. This
  work represents a significant step towards enabling efficient long-context processing
  in LLMs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen25a
month: 0
tex_title: HSR-Enhanced Sparse Attention Acceleration
firstpage: 105
lastpage: 133
page: 105-133
order: 105
cycles: false
bibtex_author: Chen, Bo and Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song,
  Zhao
author:
- given: Bo
  family: Chen
- given: Yingyu
  family: Liang
- given: Zhizhou
  family: Sha
- given: Zhenmei
  family: Shi
- given: Zhao
  family: Song
date: 2025-06-15
address:
container-title: Conference on Parsimony and Learning
volume: '280'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v280/main/assets/chen25a/chen25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
